{
    "central": {
      "name": "Hugging Face",
      "subname": "Datasets"
    },
    "items": [
      {
        "name": "FineWeb",
        "subname": "HuggingFaceFW",
        "bullets": ["Cleaned and deduplicated english web data from CommonCrawl", "93.4 TBs"],
        "description": "Contains 15T-tokens of cleaned and deduplicated english web data from CommonCrawl. Curated for large-scale LLM training. Models trained on this data show superiority over models trained on other datasets like C4, Dolma, and RedPajama.\nEstimated number of rows: 45,995,362,478.\nSize of auto-converted Parquet files: 93.4 TB. Key feature: As of today, the largest publicly available, high-quality web dataset.",
        "icon": "https://drive.google.com/file/d/14NfJZIUWspK80MKoAfLbekNcIkL4Xnjm/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/HuggingFaceFW/fineweb",
        "color": "#FFD21E",
        "x": 0.0,
        "y": -0.15
      },
      {
        "name": "orca-agentinstruct-1M-v1",
        "subname": "Microsoft",
        "bullets": ["Designed to train models for instruction-following tasks", "Prompts and responses are synthetically generated by AgentInstruct"],
        "description": "Designed to train models for instruction-following tasks like text creative writing, coding or reading comprehension. Both the prompts and the responses of this dataset are synthetically generated by AgentInstruct, using only raw text content publicly avialble on the Web as seeds.\nNumber of rows: 1,046,410\nSize of auto-converted Parquet files: 2.21 GB",
        "icon": "https://drive.google.com/file/d/1m-jM0n5aA57FbmRa7RLuXJhOC-QpDpun/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/microsoft/orca-agentinstruct-1M-v1",
        "color": "#5086BC",
        "x": 0.0,
        "y": -0.05
      },
      {
        "name": "arXiver",
        "subname": "Neuralwork",
        "bullets": ["Curated for question-answering tasks", "Data is converted to highly readable (.mmd) format"],
        "description": "The largest open and permissible licensed text dataset, comprising over 2 trillion tokens (2,003,039,184,047 tokens). Contains a diverse set of sources such as books, newspapers, scientific articles, government and legal documents, code, and more.\nEstimated number of rows: 396,953,971\nSize of auto-converted Parquet files (First 5GB): 2.96 GB\nKey feature: Data is permissively licensed, meaning it can be used, modified, and redistributed without legal ambiguity or risk of infringement.",
        "icon": "https://drive.google.com/file/d/10OJ348DMNH3nd8yJBqyP44unwQPL-x9a/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/neuralwork/arxiver",
        "color": "#2FAD3B",
        "x": 0.0,
        "y": -0.05
      },
      {
        "name": "Common Corpus",
        "subname": "PleIAs",
        "bullets": ["Permissible licensed text dataset", "Contains a diverse set of sources (books, newspapers, scientific articles)"],
        "description": "The largest open and permissible licensed text dataset, comprising over 2 trillion tokens (2,003,039,184,047 tokens). Contains a diverse set of sources such as books, newspapers, scientific articles, government and legal documents, code, and more.\nEstimated number of rows: 396,953,971\nSize of auto-converted Parquet files (First 5GB): 2.96 GB\nKey feature: Data is permissively licensed, meaning it can be used, modified, and redistributed without legal ambiguity or risk of infringement.",
        "icon": "https://drive.google.com/file/d/10OJ348DMNH3nd8yJBqyP44unwQPL-x9a/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/PleIAs/common_corpus",
        "color": "#C658C6",
        "x": 0.0,
        "y": -0.05
      },
      {
        "name": "SmolTalkDataset",
        "subname": "HuggingFaceTB",
        "bullets": ["Designed for supervised finetuning (SFT) of LLMs", "Curated to strengthen model capabilities such as mathematics and coding"],
        "description": "Synthetic dataset designed for supervised finetuning (SFT) of LLMs. It focuses on bridging the performance gap between models trained on SFT datasets and those trained on proprietary instruction datasets.\nNumber of rows: 2,197,730\nSize of the auto-converted Parquet files: 4.15 GB\nKey feature: While curated for SFT, the dataset also aims at improving on instruction following tasks.",
        "icon": "https://drive.google.com/file/d/1QhQuw1-tpEG9T0KzJjsiHQxxQqi9z9PZ/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/HuggingFaceTB/smoltalk",
        "color": "#D2E534",
        "x": 0.0,
        "y": -0.05
      },
      {
        "name": "Multilingual Massive Multitask Language Understanding (MMMLU)",
        "subname": "OpenAI",
        "bullets": ["Covers 57 topics from physics to history", "Test set translated into 14 languages using professional human translators"],
        "description": "Benchmark dataset for assessing the general knowledge and reasoning skills of AI models. It covers 57 topics from physics to history, and its test set is translated into 14 languages by professional translators to improve multilingual AI performance and accuracy.\nNumber of rows: 393,176\nSize of the auto-converted Parquet files: 124 MB\nKey feature: Aiming to improve the multilingual capabilities of AI models, ensuring they perform accurately across languages, particularly for underrepresented communities.",
        "icon": "https://drive.google.com/file/d/1pRcIEe8BgZlbig7SmB9dXeOioXlzC5IF/view?usp=drive_link",
        "url": "https://huggingface.co/datasets/openai/MMMLU",
        "color": "#323185",
        "x": 0.0,
        "y": -0.05
      }
    ]
  }
  